{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b2d01d0",
   "metadata": {},
   "source": [
    "MailGuard — Train Spam/Phishing Classifier\n",
    "Purpose: prototype preprocessing, train baseline model (Tfidf + LogisticRegression),\n",
    "evaluate, pick thresholds, and export artifacts for local Django inference.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f90bc26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\subha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.1.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\subha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\subha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\subha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\subha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\subha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\subha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy pandas joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8b6c311",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from html import unescape\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e94f19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\subha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\subha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\subha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\subha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\subha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: seaborn in c:\\users\\subha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\subha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\subha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\subha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\subha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\subha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\subha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: click in c:\\users\\subha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\subha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\subha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.67.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\subha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\subha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\subha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\subha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\subha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\subha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\subha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\subha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\subha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas scikit-learn nltk joblib matplotlib seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9f7b035",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\subha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\subha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\subha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# NLP\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix\n",
    "\n",
    "# sparse utilities\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "# Setup constants\n",
    "RANDOM_STATE = 42\n",
    "ROOT = Path('.')\n",
    "ARTIFACTS_DIR = ROOT / 'mailguard' / 'model' / 'artifacts'\n",
    "ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Ensure NLTK data (run once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "LEMMATIZER = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e116de30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\P'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\P'\n",
      "C:\\Users\\subha\\AppData\\Local\\Temp\\ipykernel_5772\\3714555922.py:1: SyntaxWarning: invalid escape sequence '\\P'\n",
      "  df = pd.read_csv(\"F:\\Projects\\MailGuard_Research_notebooks\\emails.csv\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Subject: naturally irresistible your corporate...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Subject: the stock trading gunslinger  fanny i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Subject: unbelievable new homes made easy  im ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Subject: 4 color printing special  request add...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Subject: do not have money , get software cds ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  Subject: naturally irresistible your corporate...      1\n",
       "1  Subject: the stock trading gunslinger  fanny i...      1\n",
       "2  Subject: unbelievable new homes made easy  im ...      1\n",
       "3  Subject: 4 color printing special  request add...      1\n",
       "4  Subject: do not have money , get software cds ...      1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  \n",
    "df = pd.read_csv(\"F:\\Projects\\MailGuard_Research_notebooks\\emails.csv\")\n",
    "df = df.rename(columns={ 'spam': 'label' }) if 'spam' in df.columns else df\n",
    "df = df[['text','label']].copy()  # keep only necessary columns for baseline\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f254750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before drop duplicates: 5728\n",
      "After drop duplicates: 5695\n",
      "Null counts:\n",
      " text     0\n",
      "label    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Remove exact duplicate rows\n",
    "print('Before drop duplicates:', len(df))\n",
    "df.drop_duplicates(inplace=True)\n",
    "print('After drop duplicates:', len(df))\n",
    "\n",
    "# Nulls\n",
    "print('Null counts:\\n', df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04963157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    4327\n",
      "1    1368\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a82d6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 — Mojibake & HTML-entity cleanup helpers\n",
    "MOJIBAKE_REPLACEMENTS = {\n",
    "    'ΓÇó': '-', 'ΓÇô': '-', 'ΓÇ£': '\"', 'ΓÇ¥': '\"', 'ΓÇÖ': \"'\",\n",
    "    'ΓÇü': 'u', 'â': '-', 'â': '-', 'â': '\"', 'â': \"'\",\n",
    "    'â¢': '-', 'â¢': '-', 'Ã©': 'e', '\\ufeff': ''\n",
    "}\n",
    "MOJIBAKE_REGEX = re.compile('|'.join(re.escape(k) for k in MOJIBAKE_REPLACEMENTS.keys()))\n",
    "\n",
    "URL_RE = re.compile(r'https?://\\S+|www\\.\\S+', flags=re.IGNORECASE)\n",
    "EMAIL_RE = re.compile(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b')\n",
    "PHONE_RE = re.compile(r'(\\+?\\d[\\d\\-\\s]{7,}\\d)')\n",
    "IMAGE_PLACEHOLDER_RE = re.compile(r'\\[image:[^\\]]*\\]', flags=re.IGNORECASE)\n",
    "FORWARD_MARKERS = ['forwarded message', '---------- forwarded message', 'from:']\n",
    "\n",
    "def fix_mojibake(s: str) -> str:\n",
    "    if not s:\n",
    "        return s\n",
    "    return MOJIBAKE_REGEX.sub(lambda m: MOJIBAKE_REPLACEMENTS[m.group(0)], s)\n",
    "\n",
    "def clean_text(raw: str) -> str:\n",
    "    \"\"\"Full OCR-aware cleaning:\n",
    "       - fix mojibake, unescape HTML entities\n",
    "       - remove forwarded blocks (keep only latest)\n",
    "       - replace URLs, emails, phones, image placeholders with tokens\n",
    "       - remove HTML tags, collapse whitespace, lowercase\n",
    "    \"\"\"\n",
    "    if pd.isna(raw):\n",
    "        return ''\n",
    "    s = str(raw)\n",
    "    s = fix_mojibake(s)\n",
    "    s = unescape(s)\n",
    "    # remove forwarded blocks\n",
    "    low = s.lower()\n",
    "    for m in FORWARD_MARKERS:\n",
    "        idx = low.find(m)\n",
    "        if idx != -1:\n",
    "            s = s[:idx]\n",
    "            low = s.lower()\n",
    "            break\n",
    "    # tokens\n",
    "    s = URL_RE.sub(' <URL> ', s)\n",
    "    s = EMAIL_RE.sub(' <EMAIL> ', s)\n",
    "    s = PHONE_RE.sub(' <PHONE> ', s)\n",
    "    s = IMAGE_PLACEHOLDER_RE.sub(' <IMAGE> ', s)\n",
    "    # strip html tags\n",
    "    s = re.sub(r'<[^>]+>', ' ', s)\n",
    "    # remove control chars and collapse whitespace\n",
    "    s = re.sub(r'[\\r\\n\\t]+', ' ', s)\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    return s.lower()\n",
    "\n",
    "def tokenize_and_lemmatize(text: str) -> str:\n",
    "    tokens = word_tokenize(text)\n",
    "    out = []\n",
    "    for t in tokens:\n",
    "        if t.isalpha() and t not in STOPWORDS:\n",
    "            out.append(LEMMATIZER.lemmatize(t))\n",
    "    return ' '.join(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf0cccfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 — Apply cleaning & lemmatization\n",
    "df['clean_text'] = df['text'].astype(str).map(clean_text)\n",
    "df['lemmatized_text'] = df['clean_text'].map(tokenize_and_lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae42afd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample cleaned:\n",
      "subject: naturally irresistible your corporate identity lt is really hard to recollect a company : the market is full of suqgestions and the information isoverwhelminq ; but a good catchy logo , stylish statlonery and outstanding website will make the task much easier . we do not promise that havinq\n",
      "Sample lemmatized:\n",
      "subject naturally irresistible corporate identity lt really hard recollect company market full suqgestions information isoverwhelminq good catchy logo stylish statlonery outstanding website make task \n"
     ]
    }
   ],
   "source": [
    "# Optional quick sanity\n",
    "print(\"Sample cleaned:\")\n",
    "print(df['clean_text'].iloc[0][:300])\n",
    "print(\"Sample lemmatized:\")\n",
    "print(df['lemmatized_text'].iloc[0][:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05327166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric feature sample:\n",
      "   n_urls  n_emails  has_image  text_len\n",
      "0       0         0          0      1466\n",
      "1       0         0          0       594\n",
      "2       0         0          0       439\n",
      "3       0         0          0       471\n",
      "4       0         0          0       232\n"
     ]
    }
   ],
   "source": [
    "# Cell 5 — Structured numeric features (n_urls, n_emails, has_image, text_len)\n",
    "def extract_structured_features_from_series(series_text: pd.Series) -> pd.DataFrame:\n",
    "    n_urls = series_text.str.count(URL_RE.pattern).fillna(0).astype(int)\n",
    "    n_emails = series_text.str.count(EMAIL_RE.pattern).fillna(0).astype(int)\n",
    "    has_image = series_text.str.contains(r'\\[image:|\\<image\\>', case=False, regex=True).astype(int).fillna(0).astype(int)\n",
    "    text_len = series_text.fillna('').map(len).astype(int)\n",
    "    return pd.DataFrame({\n",
    "        'n_urls': n_urls,\n",
    "        'n_emails': n_emails,\n",
    "        'has_image': has_image,\n",
    "        'text_len': text_len\n",
    "    })\n",
    "\n",
    "X_text = df['lemmatized_text']\n",
    "X_num_df = extract_structured_features_from_series(df['clean_text'])\n",
    "y = df['label'].astype(int)\n",
    "\n",
    "print(\"Numeric feature sample:\")\n",
    "print(X_num_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6bbc283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes after split:\n",
      "X_text_train: (4556,) X_num_train: (4556, 4) y_train: (4556,)\n",
      "X_text_test : (1139,) X_num_test : (1139, 4) y_test : (1139,)\n"
     ]
    }
   ],
   "source": [
    "# Cell 6 — Train/test split (reproducible & stratified)\n",
    "X_text_train, X_text_test, X_num_train, X_num_test, y_train, y_test = train_test_split(\n",
    "    X_text, X_num_df, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Reset indices\n",
    "X_text_train = X_text_train.reset_index(drop=True)\n",
    "X_text_test  = X_text_test.reset_index(drop=True)\n",
    "X_num_train  = X_num_train.reset_index(drop=True)\n",
    "X_num_test   = X_num_test.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_test  = y_test.reset_index(drop=True)\n",
    "\n",
    "print(\"Shapes after split:\")\n",
    "print(\"X_text_train:\", X_text_train.shape, \"X_num_train:\", X_num_train.shape, \"y_train:\", y_train.shape)\n",
    "print(\"X_text_test :\", X_text_test.shape,  \"X_num_test :\", X_num_test.shape,  \"y_test :\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f5d476e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF shapes: (4556, 50000) (1139, 50000)\n"
     ]
    }
   ],
   "source": [
    "# Cell 7 — TF-IDF vectorization (fit on train)\n",
    "tfidf = TfidfVectorizer(max_features=50000, ngram_range=(1,2))\n",
    "X_text_train_tfidf = tfidf.fit_transform(X_text_train.tolist())\n",
    "X_text_test_tfidf  = tfidf.transform(X_text_test.tolist())\n",
    "print(\"TF-IDF shapes:\", X_text_train_tfidf.shape, X_text_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ac9a453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final shapes (train/test): (4556, 50004) (1139, 50004)\n"
     ]
    }
   ],
   "source": [
    "# Cell 8 — Scale numeric features (fit on train)\n",
    "scaler = StandardScaler()\n",
    "X_num_train_scaled = scaler.fit_transform(X_num_train)\n",
    "X_num_test_scaled  = scaler.transform(X_num_test)\n",
    "\n",
    "# convert numeric to sparse and concat\n",
    "X_num_train_sparse = csr_matrix(X_num_train_scaled)\n",
    "X_num_test_sparse  = csr_matrix(X_num_test_scaled)\n",
    "\n",
    "X_train_final = hstack([X_text_train_tfidf, X_num_train_sparse])\n",
    "X_test_final  = hstack([X_text_test_tfidf,  X_num_test_sparse])\n",
    "\n",
    "print(\"Final shapes (train/test):\", X_train_final.shape, X_test_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e13b687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained.\n"
     ]
    }
   ],
   "source": [
    "# Cell 9 — Train classifier\n",
    "clf = LogisticRegression(max_iter=1000, class_weight='balanced', random_state=RANDOM_STATE)\n",
    "clf.fit(X_train_final, y_train)\n",
    "print(\"Model trained.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "048645fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9609929078014184\n",
      "Recall   : 0.9890510948905109\n",
      "F1       : 0.9748201438848921\n",
      "ROC AUC  : 0.9995738576431373\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99       865\n",
      "           1       0.96      0.99      0.97       274\n",
      "\n",
      "    accuracy                           0.99      1139\n",
      "   macro avg       0.98      0.99      0.98      1139\n",
      "weighted avg       0.99      0.99      0.99      1139\n",
      "\n",
      "Confusion matrix:\n",
      " [[854  11]\n",
      " [  3 271]]\n",
      "X_train shape (sparse): (4556, 50004)\n",
      "X_test  shape (sparse): (1139, 50004)\n",
      "y_train shape: (4556,)\n",
      "y_test  shape: (1139,)\n"
     ]
    }
   ],
   "source": [
    "# Cell 10 — Evaluate\n",
    "y_proba = clf.predict_proba(X_test_final)[:,1]\n",
    "y_pred  = (y_proba >= 0.5).astype(int)\n",
    "print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "print(\"Recall   :\", recall_score(y_test, y_pred))\n",
    "print(\"F1       :\", f1_score(y_test, y_pred))\n",
    "print(\"ROC AUC  :\", roc_auc_score(y_test, y_proba))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Print shapes to confirm exact X_train/X_test sizes\n",
    "print(\"X_train shape (sparse):\", X_train_final.shape)\n",
    "print(\"X_test  shape (sparse):\", X_test_final.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test  shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c06d9437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifacts saved to: mailguard\\model\\artifacts\n"
     ]
    }
   ],
   "source": [
    "# Cell 11 — Save artifacts (as you requested)\n",
    "joblib.dump(tfidf, ARTIFACTS_DIR / 'tfidf.joblib')\n",
    "joblib.dump(scaler, ARTIFACTS_DIR / 'scaler.joblib')\n",
    "joblib.dump(clf, ARTIFACTS_DIR / 'model_logreg.joblib')\n",
    "\n",
    "# Also save metadata (threshold defaults you can adjust)\n",
    "meta = {'label_map': {0:'benign',1:'spam'}, 'thresholds': {'auto_block': 0.95, 'review': 0.6}}\n",
    "with open(ARTIFACTS_DIR / 'metadata.json', 'w') as f:\n",
    "    json.dump(meta, f)\n",
    "\n",
    "print(\"Artifacts saved to:\", ARTIFACTS_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
